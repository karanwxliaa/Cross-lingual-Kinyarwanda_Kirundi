{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b354e3-dd73-4dc7-8183-75dad1f5f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import torch\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd0070-ee73-4ab4-9066-5959666ce819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import csv\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, max_length=1500):\n",
    "        self.data_path = data_path\n",
    "        self.vocabulary = list(\"\"\"abcdefghijklmnoprstuvwyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"\"\")  # Removed 'q' and 'x'\n",
    "        self.identity_mat = np.identity(len(self.vocabulary))\n",
    "        texts, labels = [], []\n",
    "        with open(data_path) as csv_file:\n",
    "            reader = csv.reader(csv_file, quotechar='\"')\n",
    "            for idx, line in enumerate(reader):\n",
    "                if idx != 0:\n",
    "                    text = \" \".join(line[1:])\n",
    "                    label = line[0]\n",
    "                    texts.append(text)\n",
    "                    labels.append(label)\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        self.length = len(self.labels)\n",
    "        self.num_classes = len(set(self.labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_text = self.texts[index]\n",
    "        data = np.array([self.identity_mat[self.vocabulary.index(i)] for i in raw_text if i in self.vocabulary],\n",
    "                        dtype=np.float32)\n",
    "        if len(data) > self.max_length:\n",
    "            data = data[:self.max_length]\n",
    "        elif len(data) < self.max_length:\n",
    "            data = np.concatenate(\n",
    "                (data, np.zeros((self.max_length - len(data), len(self.vocabulary)), dtype=np.float32)))\n",
    "        elif len(data) == 0:\n",
    "            data = np.zeros((self.max_length, len(self.vocabulary)), dtype=np.float32)\n",
    "\n",
    "        label = self.labels[index]\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        return data_tensor, label\n",
    "\n",
    "def get_evaluation(y_true, y_prob, list_metrics):\n",
    "    y_pred = np.argmax(y_prob, -1)\n",
    "    output = {}\n",
    "    if 'accuracy' in list_metrics:\n",
    "        output['accuracy'] = metrics.accuracy_score(y_true, y_pred)\n",
    "    if 'loss' in list_metrics:\n",
    "        try:\n",
    "            output['loss'] = metrics.log_loss(y_true, y_prob)\n",
    "        except ValueError:\n",
    "            output['loss'] = -1\n",
    "    if 'confusion_matrix' in list_metrics:\n",
    "        output['confusion_matrix'] = str(metrics.confusion_matrix(y_true, y_pred))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb03e7-6f19-4a50-8368-3b19f09e4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelCNN(nn.Module):\n",
    "    def __init__(self, n_classes=14, input_length=1500, input_dim=68,\n",
    "                 n_conv_filters=256,\n",
    "                 n_fc_neurons=1024):\n",
    "        super(CharacterLevelCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(input_dim, n_conv_filters, kernel_size=7, padding=0), nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3))\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(n_conv_filters, n_conv_filters, kernel_size=7, padding=0), nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(n_conv_filters, n_conv_filters, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(n_conv_filters, n_conv_filters, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(n_conv_filters, n_conv_filters, kernel_size=3, padding=0), nn.ReLU())\n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(n_conv_filters, n_conv_filters, kernel_size=3, padding=0), nn.ReLU(),\n",
    "                                   nn.MaxPool1d(3))\n",
    "        # compute the  output shape after forwarding an input to the conv layers\n",
    "        input_shape = (128,\n",
    "                      input_length,\n",
    "                      input_dim)\n",
    "        self.output_dimension = self._get_conv_output(input_shape)\n",
    "\n",
    "        self.fc1 = nn.Sequential(nn.Linear(self.output_dimension, n_fc_neurons), nn.Dropout(0.5))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(n_fc_neurons, n_fc_neurons), nn.Dropout(0.5))\n",
    "        self.fc3 = nn.Linear(n_fc_neurons, n_classes)\n",
    "\n",
    "        if n_conv_filters == 256 and n_fc_neurons == 1024:\n",
    "            self._create_weights(mean=0.0, std=0.05)\n",
    "        elif n_conv_filters == 1024 and n_fc_neurons == 2048:\n",
    "            self._create_weights(mean=0.0, std=0.02)\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output_dimension = x.size(1)\n",
    "        return output_dimension\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(1, 2)\n",
    "        output = self.conv1(input)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        output = self.conv4(output)\n",
    "        output = self.conv5(output)\n",
    "        output = self.conv6(output)\n",
    "\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.fc3(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "def train(feature, optimizer):\n",
    "    # Set the seed\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "\n",
    "    # Initialize model\n",
    "    if feature == \"small\":\n",
    "        model = CharacterLevelCNN(input_length=max_length, n_classes=14,\n",
    "                                  input_dim=len(alphabet),\n",
    "                                  n_conv_filters=256, n_fc_neurons=1024)\n",
    "    elif feature == \"large\":\n",
    "        model = CharacterLevelCNN(input_length=max_length, n_classes=14,\n",
    "                                  input_dim=len(alphabet),\n",
    "                                  n_conv_filters=1024, n_fc_neurons=2048)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature mode!\")\n",
    "\n",
    "    # Data loaders\n",
    "    training_set = MyDataset(input + \"/zero_kin_train.csv\", max_length)\n",
    "    test_set = MyDataset(input + \"/zero_kin_test.csv\", max_length)\n",
    "    training_generator = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_generator = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Move model to MPS if available\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Choose optimizer\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer type!\")\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for iter, batch in enumerate(training_generator):\n",
    "            features, label = batch\n",
    "            label = torch.Tensor(np.array(label, int)).to(device)\n",
    "            features = features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, label.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        loss_ls = []\n",
    "        te_label_ls = []\n",
    "        te_pred_ls = []\n",
    "        for batch in test_generator:\n",
    "            te_feature, te_label = batch\n",
    "            num_sample = len(te_label)\n",
    "            te_label = torch.Tensor(np.array(te_label, int)).to(device)\n",
    "            te_feature = te_feature.to(device)\n",
    "            with torch.no_grad():\n",
    "                te_predictions = model(te_feature)\n",
    "            te_loss = criterion(te_predictions, te_label.long())\n",
    "            loss_ls.append(te_loss.item() * num_sample)\n",
    "            te_label_ls.extend(te_label.cpu().numpy())\n",
    "            te_pred_ls.append(te_predictions.cpu().numpy())\n",
    "\n",
    "        te_loss = sum(loss_ls) / len(test_set)\n",
    "        te_pred = np.concatenate(te_pred_ls, 0)\n",
    "        te_label = np.array(te_label_ls)\n",
    "        test_metrics = get_evaluation(te_label, te_pred, list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {loss.item()}, Test Loss: {te_loss}, Test Accuracy: {test_metrics['accuracy']}\")\n",
    "\n",
    "        model.train()\n",
    "        if te_loss < best_loss:\n",
    "            best_loss = te_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"{output}/char-cnn_kin_{feature}.pth\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch - best_epoch > es_patience > 0:\n",
    "            print(f\"Stopping early at epoch {epoch}. Best loss was {best_loss} at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "        # Learning rate decay for SGD\n",
    "        if optimizer == \"sgd\" and epoch % 3 == 0 and epoch > 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            current_lr /= 2\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d7a91-4737-42d3-9c16-9824da6e4f4b",
   "metadata": {},
   "source": [
    "## Training on kinyarwanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84e9a5-a4ba-4a32-8ad9-e96609def007",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    alphabet = \"abcdefghijklmnoprstuvwyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"\n",
    "    max_length = 1500\n",
    "    optimizer = \"sgd\"\n",
    "    batch_size = 128\n",
    "    num_epochs = 30\n",
    "    lr = 0.001\n",
    "    es_min_delta = 0.0\n",
    "    es_patience = 3\n",
    "    input = \"cleaned\"\n",
    "    output = \"../output\"\n",
    "    log_path = \"../tensorboard/char-cnn\"\n",
    "    train(\"small\", optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f76f54-406b-4074-a7c0-7b6f3156b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the state dict\n",
    "torch.save(model.state_dict(), 'char_cnn_kinn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba388a4-e278-4693-90ad-bb270930e43c",
   "metadata": {},
   "source": [
    "## Direct evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb22c8-28c0-4130-8d46-7f69488eb684",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnoprstuvwyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"\n",
    "max_length = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cab669-985e-4c6f-9bf3-6c8a151b9c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharacterLevelCNN(input_length=1500, n_classes=14, input_dim=len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db050396-c4f8-4d50-b674-43a7d58463e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('char_cnn_kinn.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd6aed-3e12-48f6-ba3c-79a5b8cbeeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kirundi dataset\n",
    "kirundi_test_set = MyDataset(\"cleaned/zero_kir_test.csv\", max_length)\n",
    "kirundi_test_loader = DataLoader(kirundi_test_set, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f100ef5-ea47-45bb-a5a0-909821743198",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e3051-9832-4f98-b1f9-1cc1cd1ad764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    loss_ls = []\n",
    "    te_label_ls = []\n",
    "    te_pred_ls = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in test_loader:\n",
    "            features, labels = batch\n",
    "            \n",
    "            # Ensure labels are numeric\n",
    "            if isinstance(labels[0], str):  # Check if labels are strings\n",
    "                labels = np.array([int(label) for label in labels])\n",
    "            labels = torch.Tensor(labels).long()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                features = features.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss_ls.append(loss.item() * len(labels))\n",
    "            \n",
    "            te_label_ls.extend(labels.cpu().numpy())\n",
    "            te_pred_ls.append(predictions.cpu().numpy())\n",
    "    \n",
    "    total_loss = sum(loss_ls) / len(test_loader.dataset)\n",
    "    te_pred = np.concatenate(te_pred_ls, axis=0)\n",
    "    \n",
    "    return np.array(te_label_ls), te_pred, total_loss\n",
    "\n",
    "# Test and evaluate on the Kirundi dataset\n",
    "kirundi_labels, kirundi_predictions, kirundi_loss = evaluate(model, kirundi_test_loader, criterion)\n",
    "\n",
    "# Get evaluation metrics\n",
    "test_metrics = get_evaluation(kirundi_labels, kirundi_predictions, list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(kirundi_labels, np.argmax(kirundi_predictions, axis=1), average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {kirundi_loss}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']}\")\n",
    "#print(f\"Confusion Matrix:\\n{test_metrics['confusion_matrix']}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814954e-98a2-4bb6-9daa-8dfbffd1d976",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b37d66-ff86-4cc7-aaa6-0d7e0b8a9197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(feature, optimizer, model_path, fine_tune_layers=False):\n",
    "    # Set the seed\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.manual_seed(123)\n",
    "    else:\n",
    "        torch.manual_seed(123)\n",
    "\n",
    "    # Initialize model\n",
    "    if feature == \"small\":\n",
    "        model = CharacterLevelCNN(input_length=max_length, n_classes=14,\n",
    "                                  input_dim=len(alphabet),\n",
    "                                  n_conv_filters=256, n_fc_neurons=1024)\n",
    "    elif feature == \"large\":\n",
    "        model = CharacterLevelCNN(input_length=max_length, n_classes=14,\n",
    "                                  input_dim=len(alphabet),\n",
    "                                  n_conv_filters=1024, n_fc_neurons=2048)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature mode!\")\n",
    "\n",
    "    # Load the pre-trained weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Optionally freeze certain layers\n",
    "    if not fine_tune_layers:\n",
    "        for param in model.conv1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.conv2.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Add more layers as needed to freeze them\n",
    "        # For example, freeze the first few conv layers and fine-tune only fully connected layers\n",
    "\n",
    "    # Data loaders\n",
    "    training_set = MyDataset(input + \"/zero_kir_train.csv\", max_length)  # Use the fine-tuning dataset here\n",
    "    test_set = MyDataset(input + \"/zero_kir_test.csv\", max_length)       # Use the fine-tuning dataset here\n",
    "    training_generator = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    test_generator = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Move model to MPS if available\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Choose optimizer\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer type!\")\n",
    "\n",
    "    best_loss = 1e5\n",
    "    best_epoch = 0\n",
    "    model.train()\n",
    "    num_iter_per_epoch = len(training_generator)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for iter, batch in enumerate(training_generator):\n",
    "            features, label = batch\n",
    "            label = torch.Tensor(np.array(label, int)).to(device)\n",
    "            features = features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, label.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        loss_ls = []\n",
    "        te_label_ls = []\n",
    "        te_pred_ls = []\n",
    "        for batch in test_generator:\n",
    "            te_feature, te_label = batch\n",
    "            num_sample = len(te_label)\n",
    "            te_label = torch.Tensor(np.array(te_label, int)).to(device)\n",
    "            te_feature = te_feature.to(device)\n",
    "            with torch.no_grad():\n",
    "                te_predictions = model(te_feature)\n",
    "            te_loss = criterion(te_predictions, te_label.long())\n",
    "            loss_ls.append(te_loss.item() * num_sample)\n",
    "            te_label_ls.extend(te_label.cpu().numpy())\n",
    "            te_pred_ls.append(te_predictions.cpu().numpy())\n",
    "\n",
    "        te_loss = sum(loss_ls) / len(test_set)\n",
    "        te_pred = np.concatenate(te_pred_ls, 0)\n",
    "        te_label = np.array(te_label_ls)\n",
    "        test_metrics = get_evaluation(te_label, te_pred, list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}/{num_epochs}, Train Loss: {loss.item()}, Test Loss: {te_loss}, Test Accuracy: {test_metrics['accuracy']}\")\n",
    "\n",
    "        model.train()\n",
    "        if te_loss < best_loss:\n",
    "            best_loss = te_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), f\"{output}/fine_tuned_char_cnn_{feature}.pth\")\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch - best_epoch > es_patience > 0:\n",
    "            print(f\"Stopping early at epoch {epoch}. Best loss was {best_loss} at epoch {best_epoch}.\")\n",
    "            break\n",
    "\n",
    "        # Learning rate decay for SGD\n",
    "        if optimizer == \"sgd\" and epoch % 3 == 0 and epoch > 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            current_lr /= 2\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a68007-1088-4773-ba7c-3e516d71e59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Ensure reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Alphabet and parameters\n",
    "    alphabet = \"abcdefghijklmnoprstuvwyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"\n",
    "    max_length = 1500\n",
    "    optimizer = \"sgd\"\n",
    "    batch_size = 128\n",
    "    num_epochs = 15\n",
    "    lr = 0.001\n",
    "    es_min_delta = 0.0\n",
    "    es_patience = 3\n",
    "\n",
    "    # Paths to input and output\n",
    "    input = \"cleaned\"\n",
    "    output = \"../output\"\n",
    "    log_path = \"../tensorboard/char-cnn-tuned\"\n",
    "    \n",
    "    # Path to the pre-trained model you want to fine-tune\n",
    "    model_path = \"char_cnn_kinn.pth\"  # Adjust this path to where your pre-trained model is saved\n",
    "\n",
    "    # Call the fine_tune function\n",
    "    fine_tune(\n",
    "        feature=\"small\",              # Use \"small\" or \"large\" based on the model size\n",
    "        optimizer=optimizer,          # Optimizer choice (\"adam\" or \"sgd\")\n",
    "        model_path=model_path,        # Path to the pre-trained model\n",
    "        fine_tune_layers=True         # Set to False to freeze certain layers, True to fine-tune all layers\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021ae8e-cc55-4f0c-acdb-1ccfcf153182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only the state dict\n",
    "torch.save(model.state_dict(), 'char_cnn_tuned.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76bfc69-017a-4ffd-80a1-9a74f6083d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"abcdefghijklmnoprstuvwyz0123456789-,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=<>()[]{}\"\n",
    "max_length = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b32b81-c316-4850-b98e-246baffd1bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharacterLevelCNN(input_length=1500, n_classes=14, input_dim=len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf077be-d8aa-4974-a18e-995bd1e0ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('char_cnn_tuned.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4953a2-1013-4763-a416-34fe846ee78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kirundi dataset\n",
    "kirundi_test_set = MyDataset(\"cleaned/zero_kir_test.csv\", max_length)\n",
    "kirundi_test_loader = DataLoader(kirundi_test_set, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321df07-b7e8-4550-9625-214c3109d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa0160-501f-4944-a760-6e1e6c624114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    loss_ls = []\n",
    "    te_label_ls = []\n",
    "    te_pred_ls = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in test_loader:\n",
    "            features, labels = batch\n",
    "            \n",
    "            # Ensure labels are numeric\n",
    "            if isinstance(labels[0], str):  # Check if labels are strings\n",
    "                labels = np.array([int(label) for label in labels])\n",
    "            labels = torch.Tensor(labels).long()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                features = features.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss_ls.append(loss.item() * len(labels))\n",
    "            \n",
    "            te_label_ls.extend(labels.cpu().numpy())\n",
    "            te_pred_ls.append(predictions.cpu().numpy())\n",
    "    \n",
    "    total_loss = sum(loss_ls) / len(test_loader.dataset)\n",
    "    te_pred = np.concatenate(te_pred_ls, axis=0)\n",
    "    \n",
    "    return np.array(te_label_ls), te_pred, total_loss\n",
    "\n",
    "# Test and evaluate on the Kirundi dataset\n",
    "kirundi_labels, kirundi_predictions, kirundi_loss = evaluate(model, kirundi_test_loader, criterion)\n",
    "\n",
    "# Get evaluation metrics\n",
    "test_metrics = get_evaluation(kirundi_labels, kirundi_predictions, list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(kirundi_labels, np.argmax(kirundi_predictions, axis=1), average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {kirundi_loss}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']}\")\n",
    "#print(f\"Confusion Matrix:\\n{test_metrics['confusion_matrix']}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9f4b2-79ee-480b-8b78-a9431e13a2b5",
   "metadata": {},
   "source": [
    "## Forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85cd98e-ac66-4df1-863a-d190f9faf54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('char_cnn_tuned.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d19adc-b04d-4cb3-9927-bd935f9e7a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kinywarnda dataset\n",
    "kirundi_test_set = MyDataset(\"cleaned/zero_kin_test.csv\", max_length)\n",
    "kirundi_test_loader = DataLoader(kirundi_test_set, batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc877b7c-7127-463d-9860-b14a94d2f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    loss_ls = []\n",
    "    te_label_ls = []\n",
    "    te_pred_ls = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for batch in test_loader:\n",
    "            features, labels = batch\n",
    "            \n",
    "            # Ensure labels are numeric\n",
    "            if isinstance(labels[0], str):  # Check if labels are strings\n",
    "                labels = np.array([int(label) for label in labels])\n",
    "            labels = torch.Tensor(labels).long()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                features = features.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss_ls.append(loss.item() * len(labels))\n",
    "            \n",
    "            te_label_ls.extend(labels.cpu().numpy())\n",
    "            te_pred_ls.append(predictions.cpu().numpy())\n",
    "    \n",
    "    total_loss = sum(loss_ls) / len(test_loader.dataset)\n",
    "    te_pred = np.concatenate(te_pred_ls, axis=0)\n",
    "    \n",
    "    return np.array(te_label_ls), te_pred, total_loss\n",
    "\n",
    "# Test and evaluate on the Kirundi dataset\n",
    "kirundi_labels, kirundi_predictions, kirundi_loss = evaluate(model, kirundi_test_loader, criterion)\n",
    "\n",
    "# Get evaluation metrics\n",
    "test_metrics = get_evaluation(kirundi_labels, kirundi_predictions, list_metrics=[\"accuracy\", \"confusion_matrix\"])\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(kirundi_labels, np.argmax(kirundi_predictions, axis=1), average='weighted')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test Loss: {kirundi_loss}\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']}\")\n",
    "#print(f\"Confusion Matrix:\\n{test_metrics['confusion_matrix']}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee5a15-e906-469f-87dc-83b54d33fc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
