{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64ab2da-92c6-421e-8658-327a3b9f8a89",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ba8d9-2dc0-4de9-909a-e1ce9517aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import kkltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5898a-612e-4b86-9757-448be7c8729f",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d358a-fc8e-4c90-8b8e-dddc3cdf041a",
   "metadata": {},
   "source": [
    "### Kin train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6110038-a5f9-4344-a7eb-83843ea9a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kkltk.kin_kir_stopwords import stopwords   # check https://github.com/Andrews2017/kkltk for more detailed information about how to use kkltk package\n",
    "\n",
    "stopset_kin = stopwords.words('kinyarwanda') \n",
    "\n",
    "# loading the dataa\n",
    "data = pd.read_csv('KINNEWS_train.csv')\n",
    "\n",
    "# Cleaning the data (preprocessing)\n",
    "# Removing the special characters and urls\n",
    "data.title = data.title.str.replace('[^A-Za-z\\s\\’\\-]+', '')\n",
    "data.content = data.content.str.replace('[^A-Za-z\\s\\’\\-]+', '')\n",
    "data.title = data.title.str.replace('[\\n]+', '')\n",
    "data.content = data.content.str.replace('[\\n]+', '')\n",
    "data.title = data.title.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')\n",
    "data.content = data.content.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "# Removing the stopwords\n",
    "data['title'] = data['title'].apply(lambda x: ' '.join([item.lower() for item in str(x).split() if item not in stopset_kin]))\n",
    "data['content'] = data['content'].apply(lambda x: ' '.join([item.lower() for item in str(x).split() if item not in stopset_kin]))\n",
    "\n",
    "# Filter out rows where labels are 8 or 10\n",
    "data = data[~data['label'].isin([8, 10])]\n",
    "\n",
    "# Print the cleaned data\n",
    "print(data)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "data.to_csv(\"cleaned/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234cb88-35ca-4454-9353-be54305c8acc",
   "metadata": {},
   "source": [
    "### Kin - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccf5f8-f8c7-417b-9d14-c1638622e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kkltk.kin_kir_stopwords import stopwords   # check https://github.com/Andrews2017/kkltk for more detailed information about how to use kkltk package\n",
    "\n",
    "stopset_kin = stopwords.words('kinyarwanda') \n",
    "\n",
    "# loading the data\n",
    "data = pd.read_csv('KINNEWS_test.csv')\n",
    "\n",
    "# Cleaning the data (preprocessing)\n",
    "# Removing the special characters and urls\n",
    "data.title = data.title.str.replace('[^A-Za-z\\s\\’\\-]+', '')\n",
    "data.content = data.content.str.replace('[^A-Za-z\\s\\’\\-]+', '')\n",
    "data.title = data.title.str.replace('[\\n]+', '')\n",
    "data.content = data.content.str.replace('[\\n]+', '')\n",
    "data.title = data.title.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')\n",
    "data.content = data.content.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "# Removing the stopwords\n",
    "data['title'] = data['title'].apply(lambda x: ' '.join([item.lower() for item in str(x).split() if item not in stopset_kin]))\n",
    "data['content'] = data['content'].apply(lambda x: ' '.join([item.lower() for item in str(x).split() if item not in stopset_kin]))\n",
    "\n",
    "# Filter out rows where labels are 8 or 10\n",
    "data = data[~data['label'].isin([8, 10])]\n",
    "\n",
    "# Print the cleaned data\n",
    "print(data)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "data.to_csv(\"cleaned/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51ec18-8441-4bcd-818d-7755fc68884a",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c3ccf-6395-4814-ac1b-7f3e3f01df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "data_train = pd.read_csv('cleaned/train.csv')\n",
    "data_test = pd.read_csv('cleaned/test.csv')\n",
    "data = pd.concat([data_train, data_test])\n",
    "data['whole_doc'] = data['title'] + ' ' + data['content'].astype(str)\n",
    "\n",
    "# clean the data (preprocessing)\n",
    "data.whole_doc = data.whole_doc.str.replace('[^A-Za-z\\s\\’\\-]+', '')\n",
    "data.whole_doc = data.whole_doc.str.replace('[\\n]+', '')\n",
    "data.whole_doc = data.whole_doc.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "# Create the list of list format of the custom corpus for gensim modeling\n",
    "sent = [row.split(' ') for row in data['whole_doc'] if len(row)]\n",
    "sent = [[tok.lower() for tok in sub_sent if len(tok) != 0] for sub_sent in sent]\n",
    "\n",
    "# Training the model\n",
    "w2v_model = Word2Vec(sent, window=5, min_count=5, sg=1, hs=1, vector_size=50)\n",
    "\n",
    "# Generate a list of words with their vectors to make the custom embeddings generation possible\n",
    "w2v_vectors = []\n",
    "for token, idx in w2v_model.wv.key_to_index.items():\n",
    "    str_vec = ''\n",
    "    if token in w2v_model.wv.key_to_index.keys():\n",
    "        str_vec += token\n",
    "        for i in range(len(w2v_model.wv[token])):\n",
    "            str_vec += ' ' + str(w2v_model.wv[token][i])\n",
    "    w2v_vectors.append(str_vec)\n",
    "\n",
    "# Save the above embeddings list in txt file\n",
    "with open(\"W2V-Kin-50.txt\", 'w') as output:\n",
    "    for row in w2v_vectors:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcb381-f11b-444f-8447-984d369baf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "# load the data\n",
    "data_train = pd.read_csv('cleaned/kir_train.csv')\n",
    "data_test = pd.read_csv('cleaned/kir_test.csv')\n",
    "data = pd.concat([data_train, data_test])\n",
    "data['whole_doc'] = data['title'] + ' ' + data['content'].astype(str)\n",
    "\n",
    "# clean the data (preprocessing)\n",
    "data.whole_doc = data.whole_doc.str.replace('[^A-Za-z\\s\\’\\-]+', '')\n",
    "data.whole_doc = data.whole_doc.str.replace('[\\n]+', '')\n",
    "data.whole_doc = data.whole_doc.str.replace('^https?:\\/\\/.*[\\r\\n]*', '')\n",
    "\n",
    "# Create the list of list format of the custom corpus for gensim modeling\n",
    "sent = [row.split(' ') for row in data['whole_doc'] if len(row)]\n",
    "sent = [[tok.lower() for tok in sub_sent if len(tok) != 0] for sub_sent in sent]\n",
    "\n",
    "# Training the model\n",
    "w2v_model = Word2Vec(sent, window=5, min_count=5, sg=1, hs=1, vector_size=50)\n",
    "\n",
    "# Generate a list of words with their vectors to make the custom embeddings generation possible\n",
    "w2v_vectors = []\n",
    "for token, idx in w2v_model.wv.key_to_index.items():\n",
    "    str_vec = ''\n",
    "    if token in w2v_model.wv.key_to_index.keys():\n",
    "        str_vec += token\n",
    "        for i in range(len(w2v_model.wv[token])):\n",
    "            str_vec += ' ' + str(w2v_model.wv[token][i])\n",
    "    w2v_vectors.append(str_vec)\n",
    "\n",
    "# Save the above embeddings list in txt file\n",
    "with open(\"W2V-Kir-50.txt\", 'w') as output:\n",
    "    for row in w2v_vectors:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ea671-a339-4661-93c2-317e748ee006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
